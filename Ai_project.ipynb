{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsVEfvbwy3fm+8E5/WiG9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CalebEng/Ai-Project-learning/blob/master/Ai_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4egB2tLlsSDE",
        "outputId": "3b7e8763-0941-484f-e5a3-fd15ba329f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "!pip install --upgrade tiktoken\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Ai work')\n",
        "#torch.__version__\n",
        "enc = tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "------------------- Prep and loading data sets -------------------"
      ],
      "metadata": {
        "id": "o_uZv-_ztqwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--ONLY RUN THE NEXT IF CSV FILE CONTAINING CONVOS HAS A INDEX COLUMN--"
      ],
      "metadata": {
        "id": "oNmCxGdbtT3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('poems.csv')\n",
        "# If you know the name of the column skip this\n",
        "first_column = df.columns[0]\n",
        "sec_col = df.columns[2]\n",
        "trd_col = df.columns[3]\n",
        "four_col = df.columns[4]\n",
        "\n",
        "# Delete first\n",
        "df = df.drop([first_column], axis=1)\n",
        "df = df.drop([sec_col],axis=1)\n",
        "df = df.drop([trd_col],axis = 1)\n",
        "df = df.drop([four_col],axis = 1)\n",
        "\n",
        "\n",
        "df.to_csv('poem.csv',index = False)\n"
      ],
      "metadata": {
        "id": "JRj3gf5atSd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('poems.csv',index=False)"
      ],
      "metadata": {
        "id": "W-NrTONA3WSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('poem.csv')"
      ],
      "metadata": {
        "id": "Qauky9fH7xfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stdf = df['content'].tolist()"
      ],
      "metadata": {
        "id": "280LNekcsPtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df2 = pd.read_csv('Conversation.csv')\n",
        "\n",
        "questions = df2['question'].tolist()\n",
        "answers = df2['answer'].tolist()\n",
        "\n",
        "temp = map(format, questions)\n",
        "temp2 = map(format, answers)\n",
        "\n",
        "questions = list(temp)\n",
        "answers = list(temp2)\n"
      ],
      "metadata": {
        "id": "L-nijJSCtytS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--EXIT--"
      ],
      "metadata": {
        "id": "AMfwp0hitdK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poems=\"\"\n",
        "for i in stdf:\n",
        "  poems+=i\n",
        "  poems+=\"\\n\\n\\n\"\n",
        "poems=poems[:-1]\n",
        "\n",
        "print(poems[:10000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdfWckO6PYWG",
        "outputId": "ce7ec463-f310-4a98-a5e8-1ebe0c454c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let the bird of loudest lay\r\n",
            "On the sole Arabian tree\r\n",
            "Herald sad and trumpet be,\r\n",
            "To whose sound chaste wings obey.\r\n",
            "\r\n",
            "But thou shrieking harbinger,\r\n",
            "Foul precurrer of the fiend,\r\n",
            "Augur of the fever's end,\r\n",
            "To this troop come thou not near.\r\n",
            "\r\n",
            "From this session interdict\r\n",
            "Every fowl of tyrant wing,\r\n",
            "Save the eagle, feather'd king;\r\n",
            "Keep the obsequy so strict.\r\n",
            "\r\n",
            "Let the priest in surplice white,\r\n",
            "That defunctive music can,\r\n",
            "Be the death-divining swan,\r\n",
            "Lest the requiem lack his right.\r\n",
            "\r\n",
            "And thou treble-dated crow,\r\n",
            "That thy sable gender mak'st\r\n",
            "With the breath thou giv'st and tak'st,\r\n",
            "'Mongst our mourners shalt thou go.\r\n",
            "\r\n",
            "Here the anthem doth commence:\r\n",
            "Love and constancy is dead;\r\n",
            "Phoenix and the Turtle fled\r\n",
            "In a mutual flame from hence.\r\n",
            "\r\n",
            "So they lov'd, as love in twain\r\n",
            "Had the essence but in one;\r\n",
            "Two distincts, division none:\r\n",
            "Number there in love was slain.\r\n",
            "\r\n",
            "Hearts remote, yet not asunder;\r\n",
            "Distance and no space was seen\r\n",
            "'Twixt this Turtle and his queen:\r\n",
            "But in them it were a wonder.\r\n",
            "\r\n",
            "So between them love did shine\r\n",
            "That the Turtle saw his right\r\n",
            "Flaming in the Phoenix' sight:\r\n",
            "Either was the other's mine.\r\n",
            "\r\n",
            "Property was thus appalled\r\n",
            "That the self was not the same;\r\n",
            "Single nature's double name\r\n",
            "Neither two nor one was called.\r\n",
            "\r\n",
            "Reason, in itself confounded,\r\n",
            "Saw division grow together,\r\n",
            "To themselves yet either neither,\r\n",
            "Simple were so well compounded;\r\n",
            "\r\n",
            "That it cried, \"How true a twain\r\n",
            "Seemeth this concordant one!\r\n",
            "Love has reason, reason none,\r\n",
            "If what parts can so remain.\"\r\n",
            "\r\n",
            "Whereupon it made this threne\r\n",
            "To the Phoenix and the Dove,\r\n",
            "Co-supremes and stars of love,\r\n",
            "As chorus to their tragic scene:\r\n",
            "\r\n",
            "                 threnos\r\n",
            "\r\n",
            "Beauty, truth, and rarity,\r\n",
            "Grace in all simplicity,\r\n",
            "Here enclos'd, in cinders lie.\r\n",
            "\r\n",
            "Death is now the Phoenix' nest,\r\n",
            "And the Turtle's loyal breast\r\n",
            "To eternity doth rest,\r\n",
            "\r\n",
            "Leaving no posterity:\r\n",
            "'Twas not their infirmity,\r\n",
            "It was married chastity.\r\n",
            "\r\n",
            "Truth may seem but cannot be;\r\n",
            "Beauty brag but 'tis not she;\r\n",
            "Truth and beauty buried be.\r\n",
            "\r\n",
            "To this urn let those repair\r\n",
            "That are either true or fair;\r\n",
            "For these dead birds sigh a prayer.\n",
            "\n",
            "\n",
            "Sir Charles into my chamber coming in,\r\n",
            "When I was writing of my Fairy Queen;\r\n",
            "I praysaid hewhen Queen Mab you do see\r\n",
            "Present my service to her Majesty:\r\n",
            "And tell her I have heard Fame's loud report\r\n",
            "Both of her beauty and her stately court.\r\n",
            "When I Queen Mab within my fancy viewed,\r\n",
            "My thoughts bowed low, fearing I should be rude;\r\n",
            "Kissing her garment thin which fancy made,\r\n",
            "I knelt upon a thought, like one that prayed;\r\n",
            "And then, in whispers soft, I did present\r\n",
            "His humble service which in mirth was sent;\r\n",
            "Thus by imagination I have been\r\n",
            "In Fairy court and seen the Fairy Queen.\n",
            "\n",
            "\n",
            "Our vice runs beyond all that old men saw,\r\n",
            "And far authentically above our laws,\r\n",
            "And scorning virtues safe and golden mean,\r\n",
            "Sits uncontrolled upon the high extreme.\r\n",
            "Circes, thy monsters painted out the hue,\r\n",
            "Of feigned filthiness, but ours is true.\r\n",
            "Our vice puts down all proverbs and all themes,\r\n",
            "Our vice excels all fables and all dreams.\n",
            "\n",
            "\n",
            "Lo I the man, whose Muse whilome did maske,\r\n",
            "As time her taught in lowly Shepheards weeds,\r\n",
            "Am now enforst a far unfitter taske,\r\n",
            "For trumpets sterne to chaunge mine Oaten reeds,\r\n",
            "And sing of Knights and Ladies gentle deeds;\r\n",
            "Whose prayses having slept in silence long,\r\n",
            "Me, all too meane, the sacred Muse areeds\r\n",
            "To blazon broad emongst her learned throng:\r\n",
            "Fierce warres and faithful loves shall moralize my song.\r\n",
            "Helpe then, O holy Virgin chiefe of nine,\r\n",
            "Thy weaker Novice to performe thy will,\r\n",
            "Lay forth out of thine everlasting scryne\r\n",
            "The antique rolles, which there lye hidden still,\r\n",
            "Of Faerie knights and fairest Tanaquill,\r\n",
            "Whom that most noble Briton Prince so long\r\n",
            "Sought through the world, and suffered so much ill,\r\n",
            "That I must rue his undeserved wrong:\r\n",
            "O helpe thou my weake wit, and sharpen my dull tong.\r\n",
            "And thou most dreaded impe of highest Jove,\r\n",
            "Faire Venus sonne, that with thy cruell dart\r\n",
            "At that good knight so cunningly didst rove,\r\n",
            "That glorious fire it kindled in his hart,\r\n",
            "Lay now thy deadly Heben bow apart,\r\n",
            "And with thy mother milde come to mine ayde:\r\n",
            "Come both, and with you bring triumphant Mart,\r\n",
            "In loves and gentle jollities arrayd,\r\n",
            "After his murdrous spoiles and bloudy rage allayd.\r\n",
            "And with them eke, O Goddesse heavenly bright,\r\n",
            "Mirrour of grace and Majestie divine,\r\n",
            "Great Lady of the greatest Isle, whose light\r\n",
            "Like Phoebus lampe throughout the world doth shine,\r\n",
            "Shed thy faire beames into my feeble eyne,\r\n",
            "And raise my thoughts too humble and too vile,\r\n",
            "To thinke of that true glorious type of thine,\r\n",
            "The argument of mine afflicted stile:\r\n",
            "The which to heare, vouchsafe, O dearest dred a-while.\r\n",
            "\r\n",
            "i\r\n",
            "A Gentle Knight was pricking on the plaine,\r\n",
            "Y cladd in mightie armes and silver shielde,\r\n",
            "Wherein old dints of deepe wounds did remaine,\r\n",
            "The cruell markes of many a bloudy fielde;\r\n",
            "Yet armes till that time did he never wield:\r\n",
            "His angry steede did chide his foming bitt,\r\n",
            "As much disdayning to the curbe to yield:\r\n",
            "Full jolly knight he seemd, and faire did sitt,\r\n",
            "As one for knightly giusts and fierce encounters fitt.\r\n",
            "\r\n",
            "ii\r\n",
            "But on his brest a bloudie Crosse he bore,\r\n",
            "The deare remembrance of his dying Lord,\r\n",
            "For whose sweete sake that glorious badge he wore,\r\n",
            "And dead as living ever him ador'd:\r\n",
            "Upon his shield the like was also scor'd,\r\n",
            "For soveraine hope, which in his helpe he had:\r\n",
            "Right faithfull true he was in deede and word,\r\n",
            "But of his cheere did seeme too solemne sad;\r\n",
            "Yet nothing did he dread, but ever was ydrad.\r\n",
            "\r\n",
            "iii\r\n",
            "Upon a great adventure he was bond,\r\n",
            "That greatest Gloriana to him gave,\r\n",
            "That greatest Glorious Queene of Faerie lond,\r\n",
            "To winne him worship, and her grace to have,\r\n",
            "Which of all earthly things he most did crave;\r\n",
            "And ever as he rode, his hart did earne\r\n",
            "To prove his puissance in battell brave\r\n",
            "Upon his foe, and his new force to learne;\r\n",
            "Upon his foe, a Dragon horrible and stearne.\r\n",
            "\r\n",
            "iv\r\n",
            "A lovely Ladie rode him faire beside,\r\n",
            "Upon a lowly Asse more white then snow,\r\n",
            "Yet she much whiter, but the same did hide\r\n",
            "Under a vele, that wimpled was full low,\r\n",
            "And over all a blacke stole she did throw,\r\n",
            "As one that inly mournd: so was she sad,\r\n",
            "And heavie sat upon her palfrey slow;\r\n",
            "Seemed in heart some hidden care she had,\r\n",
            "And by her in a line a milke white lambe she lad.\r\n",
            "\r\n",
            "v\r\n",
            "So pure an innocent, as that same lambe,\r\n",
            "She was in life and every vertuous lore,\r\n",
            "And by descent from Royall lynage came\r\n",
            "Of ancient Kings and Queenes, that had of yore\r\n",
            "Their scepters stretcht from East to Westerne shore,\r\n",
            "And all the world in their subjection held;\r\n",
            "Till that infernall feend with foule uprore\r\n",
            "Forwasted all their land, and them expeld:\r\n",
            "Whom to avenge, she had this Knight from far compeld.\r\n",
            "\r\n",
            "vi\r\n",
            "Behind her farre away a Dwarfe did lag,\r\n",
            "That lasie seemd in being ever last,\r\n",
            "Or wearied with bearing of her bag\r\n",
            "Of needments at his backe. Thus as they past,\r\n",
            "The day with cloudes was suddeine overcast,\r\n",
            "And angry Jove an hideous storme of raine\r\n",
            "Did poure into his Lemans lap so fast,\r\n",
            "That every wight to shrowd it did constrain,\r\n",
            "And this faire couple eke to shroud themselves were fain.\r\n",
            "\r\n",
            "vii\r\n",
            "Enforst to seeke some covert nigh at hand,\r\n",
            "A shadie grove not far away they spide,\r\n",
            "That promist ayde the tempest to withstand:\r\n",
            "Whose loftie trees yclad with sommers pride,\r\n",
            "Did spred so broad, that heavens light did hide,\r\n",
            "Not perceable with power of any starre:\r\n",
            "And all within were pathes and alleies wide,\r\n",
            "With footing worne, and leading inward farre:\r\n",
            "Faire harbour that them seemes; so in they entred arre.\r\n",
            "\r\n",
            "viii\r\n",
            "And foorth they passe, with pleasure forward led,\r\n",
            "Joying to heare the birdes sweete harmony,\r\n",
            "Which therein shrouded from the tempest dred,\r\n",
            "Seemd in their song to scorne the cruell sky.\r\n",
            "Much can they prayse the trees so straight and hy,\r\n",
            "The sayling Pine, the Cedar proud and tall,\r\n",
            "The vine-prop Elme, the Poplar never dry,\r\n",
            "The builder Oake, sole king of forrests all,\r\n",
            "The Aspine good for staves, the Cypresse funerall.\r\n",
            "\r\n",
            "ix\r\n",
            "The Laurell, meed of mightie Conquerours\r\n",
            "And Poets sage, the Firre that weepeth still,\r\n",
            "The Willow worne of forlorne Paramours,\r\n",
            "The Eugh obedient to the benders will,\r\n",
            "The Birch for shaftes, the Sallow for the mill,\r\n",
            "The Mirrhe sweete bleeding in the bitter wound,\r\n",
            "The warlike Beech, the Ash for nothing ill,\r\n",
            "The fruitfull Olive, and the Platane round,\r\n",
            "The carver Holme, the Maple seeldom inward sound.\r\n",
            "\r\n",
            "x\r\n",
            "Led with delight, they thus beguile the way,\r\n",
            "Untill the blustring storme is overblowne;\r\n",
            "When weening to returne, whence they did stray,\r\n",
            "They cannot find that path, which first was showne,\r\n",
            "But wander too and fro in wayes unknowne,\r\n",
            "Furthest from end then, when they neerest weene,\r\n",
            "That makes them doubt, their wits be not their owne:\r\n",
            "So many pathes, so many turnings seene,\r\n",
            "That which of them to take, in diverse doubt they been.\r\n",
            "\r\n",
            "xi\r\n",
            "At last resolving forward still to fare,\r\n",
            "Till that some end they finde or in or out,\r\n",
            "That path they take, that beaten seemd most bare,\r\n",
            "And like to lead the labyrinth about;\r\n",
            "Which when by tract they hunted had throughout,\r\n",
            "At length it brought them to a hollow cave,\r\n",
            "Amid the thickest woods. The Champion stout\r\n",
            "Eftsoones dismounted from his courser brave,\r\n",
            "And to the Dwarfe a while his needlesse spere he gave.\r\n",
            "\r\n",
            "xii\r\n",
            "Be well aware, quoth then that Ladie milde,\r\n",
            "Least suddaine mischiefe ye too rash provoke:\r\n",
            "The danger hid, the place unknowne and wilde,\r\n",
            "Breeds dreadfull doubts: Oft fire is without smoke,\r\n",
            "And perill without show: therefore your stroke\r\n",
            "Sir knight with-hold, till further triall made.\r\n",
            "Ah Ladie (said he) shame were to revoke\r\n",
            "The forward footing for an hidden shade:\r\n",
            "Vertue gives her selfe light, through darkenesse for to wade.\r\n",
            "\r\n",
            "xiii\r\n",
            "Yea but (quoth she) the perill of this place\r\n",
            "I better wot then you, though now too late\r\n",
            "To wish you backe returne with foule disgrace,\r\n",
            "Yet wisedome warnes, whilest foot is in the gate,\r\n",
            "To stay the steppe, ere forced to retrate.\r\n",
            "This is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--Padding and turning data into tensors--"
      ],
      "metadata": {
        "id": "b6xGSVoveDLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Conversation.csv')\n",
        "\n",
        "temp1 = df['question'].tolist()\n",
        "temp2 = df['answer'].tolist()\n",
        "\n",
        "def format(x):\n",
        "  y=enc.encode(x)\n",
        "  return y\n",
        "\n",
        "qT = list(map(format, temp1))\n",
        "aT = list(map(format, temp2))\n",
        "\n",
        "for i in range(0,len(qT)):\n",
        "  if len(qT[i])<25:\n",
        "    while len(qT[i])<25:\n",
        "      qT[i].append(482)\n",
        "\n",
        "\n",
        "for i in range(0,len(aT)):\n",
        "  if len(aT[i])<25:\n",
        "    while len(aT[i])<25:\n",
        "      aT[i].append(482)\n",
        "\n",
        "lists = [qT,aT]\n",
        "data = [val for tup in zip(*lists) for val in tup]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4YPYs4MvYViI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encPoems = enc.encode(poems)\n",
        "\n"
      ],
      "metadata": {
        "id": "zmXTPPjkNPpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tenData = torch.tensor(encPoems)"
      ],
      "metadata": {
        "id": "fz6Nfpa3da4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tenData.type\n"
      ],
      "metadata": {
        "id": "tqQon1ys-ZJd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b96cad3-2dbe-4d9f-8034-a61a5dfaeef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function Tensor.type>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(tenData))\n",
        "\n",
        "train_data = tenData[:n]\n",
        "\n",
        "val_data = tenData[n:]\n"
      ],
      "metadata": {
        "id": "GHo_ucFT-AeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n"
      ],
      "metadata": {
        "id": "4cd5NyAJqBeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = x[t]\n",
        "  print(f\"when input is {context} \\nthe target is: {target}\")"
      ],
      "metadata": {
        "id": "0MMjyxozsqD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0795430-c382-43ee-dc26-92c0ce42f9ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([10267]) \n",
            "the target is: 10267\n",
            "when input is tensor([10267,   279]) \n",
            "the target is: 279\n",
            "when input is tensor([10267,   279, 12224]) \n",
            "the target is: 12224\n",
            "when input is tensor([10267,   279, 12224,   315]) \n",
            "the target is: 315\n",
            "when input is tensor([10267,   279, 12224,   315, 29740]) \n",
            "the target is: 29740\n",
            "when input is tensor([10267,   279, 12224,   315, 29740,  5086]) \n",
            "the target is: 5086\n",
            "when input is tensor([10267,   279, 12224,   315, 29740,  5086, 11203]) \n",
            "the target is: 11203\n",
            "when input is tensor([10267,   279, 12224,   315, 29740,  5086, 11203,   319]) \n",
            "the target is: 319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "JJhI8Zljuozt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6130b5-8e1b-434b-d134-0ae4ba33ad9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 3805,   319, 59522, 27027,  1901, 97353, 20962,  1550],\n",
            "        [  361,  8617,   430,  2316,   656,   339,   834, 35563],\n",
            "        [  389,  1077,  1093,   264, 59380,  3304,  2675,  1518],\n",
            "        [ 2967,   347,    11,   779,   606,   439,   872, 17457]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[  319, 59522, 27027,  1901, 97353, 20962,  1550, 57150],\n",
            "        [ 8617,   430,  2316,   656,   339,   834, 35563,  1909],\n",
            "        [ 1077,  1093,   264, 59380,  3304,  2675,  1518,   279],\n",
            "        [  347,    11,   779,   606,   439,   872, 17457,   409]])\n",
            "----\n",
            "when input is [3805] the target: 319\n",
            "when input is [3805, 319] the target: 59522\n",
            "when input is [3805, 319, 59522] the target: 27027\n",
            "when input is [3805, 319, 59522, 27027] the target: 1901\n",
            "when input is [3805, 319, 59522, 27027, 1901] the target: 97353\n",
            "when input is [3805, 319, 59522, 27027, 1901, 97353] the target: 20962\n",
            "when input is [3805, 319, 59522, 27027, 1901, 97353, 20962] the target: 1550\n",
            "when input is [3805, 319, 59522, 27027, 1901, 97353, 20962, 1550] the target: 57150\n",
            "when input is [361] the target: 8617\n",
            "when input is [361, 8617] the target: 430\n",
            "when input is [361, 8617, 430] the target: 2316\n",
            "when input is [361, 8617, 430, 2316] the target: 656\n",
            "when input is [361, 8617, 430, 2316, 656] the target: 339\n",
            "when input is [361, 8617, 430, 2316, 656, 339] the target: 834\n",
            "when input is [361, 8617, 430, 2316, 656, 339, 834] the target: 35563\n",
            "when input is [361, 8617, 430, 2316, 656, 339, 834, 35563] the target: 1909\n",
            "when input is [389] the target: 1077\n",
            "when input is [389, 1077] the target: 1093\n",
            "when input is [389, 1077, 1093] the target: 264\n",
            "when input is [389, 1077, 1093, 264] the target: 59380\n",
            "when input is [389, 1077, 1093, 264, 59380] the target: 3304\n",
            "when input is [389, 1077, 1093, 264, 59380, 3304] the target: 2675\n",
            "when input is [389, 1077, 1093, 264, 59380, 3304, 2675] the target: 1518\n",
            "when input is [389, 1077, 1093, 264, 59380, 3304, 2675, 1518] the target: 279\n",
            "when input is [2967] the target: 347\n",
            "when input is [2967, 347] the target: 11\n",
            "when input is [2967, 347, 11] the target: 779\n",
            "when input is [2967, 347, 11, 779] the target: 606\n",
            "when input is [2967, 347, 11, 779, 606] the target: 439\n",
            "when input is [2967, 347, 11, 779, 606, 439] the target: 872\n",
            "when input is [2967, 347, 11, 779, 606, 439, 872] the target: 17457\n",
            "when input is [2967, 347, 11, 779, 606, 439, 872, 17457] the target: 409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "id": "BNxTF9UZVvf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a5e864-3cd3-437f-fbfd-ac1788dac908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3805,   319, 59522, 27027,  1901, 97353, 20962,  1550],\n",
            "        [  361,  8617,   430,  2316,   656,   339,   834, 35563],\n",
            "        [  389,  1077,  1093,   264, 59380,  3304,  2675,  1518],\n",
            "        [ 2967,   347,    11,   779,   606,   439,   872, 17457]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL** **TIME**"
      ],
      "metadata": {
        "id": "ju7QniTZRErX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramLanguageModeler(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "\n",
        "        # embedding layer contains embeddings for each word\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # linear layers to extract patterns from the corpus\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "X_VPeizHO-jz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in trigrams:\n",
        "        context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
        "        model.zero_grad()\n",
        "        log_probs = model(context_idxs)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_idx[target]], dtype=torch.long))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "x0JEHkBzS357",
        "outputId": "8712b692-43e9-4277-d4dc-276bd849a179"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1864c753be43>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBigramLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-4b0eed512df6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# idx and targets are both (B,T) tensor of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    }
  ]
}